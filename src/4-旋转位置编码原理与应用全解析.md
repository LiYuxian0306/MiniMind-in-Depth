## 位置编码区别分析：Sinusoidal PE vs. RoPE

思路来源于(sin pe问题讨论)[https://cloud.tencent.com/developer/article/2403895]   

#### 符号定义

*   $x_m, x_n \in \mathbb{R}^{d_{model}}$：位置 $m$ 和 $n$ 的输入词向量。
*   $W_q, W_k \in \mathbb{R}^{d_{head} \times d_{model}}$：Query 和 Key 的投影矩阵。
*   $A_{m,n}$：位置 $m$ 对位置 $n$ 的 Attention Score（未归一化的 Logits）。
*   $p_m, p_n$：绝对位置向量（用于 Sinusoidal PE）。
*   $R_m, R_n$：旋转矩阵（用于 RoPE）。

---

### 1. Sinusoidal PE (加法策略)

原始 Transformer 采用的方法，位置信息在投影前被**加**到输入上。

#### 1.1 输入构造  

$$
\begin{aligned}
\text{Input}_m &= x_m + p_m \\
\text{Input}_n &= x_n + p_n
\end{aligned}
$$

#### 1.2 投影 (Projection)  

$$
\begin{aligned}
q_m &= W_q (x_m + p_m) \\
k_n &= W_k (x_n + p_n)
\end{aligned}
$$

#### 1.3 Attention Score 推导
令 $W_{qk} = W_q^T W_k$ ，计算内积：
  
$$
\begin{aligned}
A_{m,n} &= q_m^T k_n \\
&= (x_m + p_m)^T W_q^T W_k (x_n + p_n) \\
&= (x_m + p_m)^T W_{qk} (x_n + p_n)
\end{aligned}
$$

展开后得到**四项**：
  
$$
A_{m,n} = \underbrace{x_m^T W_{qk} x_n}_{\text{内容-内容}} + \underbrace{x_m^T W_{qk} p_n}_{\text{内容-位置偏置}} + \underbrace{p_m^T W_{qk} x_n}_{\text{位置-内容偏置}} + \underbrace{p_m^T W_{qk} p_n}_{\text{位置-位置}}
$$

#### 1.4 核心分析
*   **信息混合**：中间两项 ( $x^T W p$ ) 混合了语义内容与绝对位置，对于仅需相对距离的任务而言，这些是难以消除的噪音。
*   **权重依赖**：相对位置信息主要存在于 $p_m^T W_{qk} p_n$ 中，但这要求模型学习出的 $W_{qk}$ 必须能保留正弦函数的相对性特征，增加了学习难度。

---

### 2. RoPE (旋转策略)

RoPE 在投影**之后**、计算内积**之前**，通过**旋转**注入位置信息。

#### 2.1 投影 (无位置信息)  

$$
\begin{aligned}
\tilde{q}_m &= W_q x_m \\
\tilde{k}_n &= W_k x_n
\end{aligned}
$$

#### 2.2 注入位置 (Rotation)
应用分块对角旋转矩阵 $R_m, R_n$。
*性质*：$R_m$ 是正交矩阵 ( $R_m^T = R_m^{-1}$ )，且 $R_m^T = R_{-m}$ 。

$$
\begin{aligned}
q_m &= R_m \tilde{q}_m = R_m W_q x_m \\
k_n &= R_n \tilde{k}_n = R_n W_k x_n
\end{aligned}
$$

#### 2.3 Attention Score 推导  

$$
\begin{aligned}
A_{m,n} &= q_m^T k_n \\
&= (R_m W_q x_m)^T (R_n W_k x_n) \\
&= x_m^T W_q^T R_m^T R_n W_k x_n
\end{aligned}
$$

#### 2.4 相对位置的显现
利用旋转矩阵几何性质： $R_m^T R_n = R_{n-m}$ （转 $n$ 度再回转 $m$ 度 等于 转 $n-m$ 度）。

$$
A_{m,n} = x_m^T W_q^T R_{n-m} W_k x_n
$$

#### 2.5 核心分析
*   **纯粹的相对性**：绝对位置 $m$ 和 $n$ 在最终公式中完全消失，Attention Score 仅取决于相对距离 $(n-m)$ 。
*   **无交叉项噪音**：不同于 Sinusoidal PE 的四项展开，RoPE 只有一项。没有“内容-绝对位置”的干扰。
*   **解耦**： $W$ 矩阵专注于提取语义特征， $R$ 矩阵专注于几何位置调制，两者互不干扰。

---

### 3. 总结对比

| 特性 | Sinusoidal PE | RoPE |
| :--- | :--- | :--- |
| **操作逻辑** | 加法 ( $+$ ) | 乘法 / 旋转 ( $\times$ ) |
| **注入时机** | 投影前 (Before Projection) | 投影后 (After Projection) |
| **最终公式** | $A_{m,n} = (x_m+p_m)^T W_{qk} (x_n+p_n)$ | $A_{m,n} = (W_q x_m)^T R_{n-m} (W_k x_n)$ |
| **展开项数** | **4项** (含噪音交叉项) | **1项** (纯净的旋转调制) |
| **相对位置** | 隐式 (依赖 $W_{qk}$ 的学习) | 显式 (数学恒等式保证) |  



      


RoPE的目标，正是找到一种旋转操作，使得在不显式计算位置差 $m-n$ 的情况下，位置编码自然地将“相对位置信息”融入到注意力机制中的$qk$中。换句话说，需要找到这么一种映射 $g$ ，针对给定的两个用于计算注意力的向量 $q$ 和 $k$ ，以及m-n，使得
$$f(q,m)f(k,n)=g(q,k,m-n)$$

其中，$q$和$k$是长度为$d_{model}$的向量，m和n分别是对应向量中第m个和第n个pos处的元素。


# 一、回顾二维向量的旋转操作
根据线性代数的知识，二维向量的旋转操作，指的是对该二维向量施加一个旋转矩阵变换，变换前后只改变二维向量的方向而保持其模长不变。

给定一个二维向量：

$$
\mathbf{x} = \begin{bmatrix} x \\ y \end{bmatrix}
$$

我们希望将它在二维平面上**逆时针旋转**一个角度 $\theta$，可以通过乘以旋转矩阵来实现：

$$
\mathbf{x}_{\text{rot}} = R(\theta) \cdot \mathbf{x}
$$

其中旋转矩阵 $R(\theta)$ 为：

$$
R(\theta) = 
\begin{bmatrix}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{bmatrix}
$$



举个实际例子，若 $$\mathbf{x} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$$ 且 $$\theta = \frac{\pi}{2}$$

（即逆时针旋转 90°）：

$$
\mathbf{x}_{\text{rot}} = 
\begin{bmatrix}
\cos \frac{\pi}{2} & -\sin \frac{\pi}{2} \\
\sin \frac{\pi}{2} & \cos \frac{\pi}{2}
\end{bmatrix}
\begin{bmatrix}
1 \\
0
\end{bmatrix}
=
\begin{bmatrix}
0 \\
1
\end{bmatrix}
$$


即：向量从 $$(1, 0)$$ 旋转为 $$(0, 1)$$

用代码可视化上述旋转过程，如下：

```python
import numpy as np
import matplotlib.pyplot as plt

def rotate_2d(x, y, theta_rad):
    R = np.array([
        [np.cos(theta_rad), -np.sin(theta_rad)],
        [np.sin(theta_rad),  np.cos(theta_rad)]
    ])
    vec = np.array([x, y])
    return R @ vec

# 原始向量
x0, y0 = 1, 0

# 旋转角度（单位：弧度）
theta_deg = 90
theta_rad = np.deg2rad(theta_deg)

# 旋转后向量
x1, y1 = rotate_2d(x0, y0, theta_rad)

# 可视化
plt.figure(figsize=(6, 6))
plt.quiver(0, 0, x0, y0, angles='xy', scale_units='xy', scale=1, color='blue', label='original')
plt.quiver(0, 0, x1, y1, angles='xy', scale_units='xy', scale=1, color='red', label=f'rotate{theta_deg}° ')

# 坐标轴设置
plt.xlim(-1.5, 1.5)
plt.ylim(-1.5, 1.5)
plt.gca().set_aspect('equal')
plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)
plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)
plt.grid(True)
plt.legend()

plt.show()
```

![](1.png)

# 二、RoPE的工作原理（d_model=2）
前面说过，RoPE 是将位置信息通过旋转操作直接注入到注意力机制中的 $q$ 和 $k$ 向量中。

假设：

- 模型维度：$d_{\text{model}} = 2$
- 原始向量：
  $$
  \mathbf{q} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \quad \mathbf{k} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}
  $$
- 序列位置：设为 $\text{pos}_q = m$，$\text{pos}_k = n$
- 对应位置角频率为 $\theta = \omega \cdot \text{pos}$，其中 $\omega$ 是一个频率超参数

---

### Step 1: 对 q 和 k 分别旋转

定义二维旋转操作为：

$$
\text{RoPE}(\mathbf{x}, \theta) = 
\begin{bmatrix}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{bmatrix}
\cdot \mathbf{x}
$$

分别对 $\mathbf{q}$ 和 $\mathbf{k}$ 施加旋转：

- 设 $\theta_q = \omega \cdot m,\quad \theta_k = \omega \cdot n$
- 旋转后的向量为：

$$
\tilde{\mathbf{q}} = R(\theta_q)\cdot \mathbf{q}, \quad 
\tilde{\mathbf{k}} = R(\theta_k)\cdot \mathbf{k}
$$

---

### Step 2: 点积操作变成了**相对位置信息的函数**

旋转后计算注意力时，执行的是：

$$
\tilde{\mathbf{q}}^\top \cdot \tilde{\mathbf{k}} = \mathbf{q}^\top R(-\theta_q) R(\theta_k) \cdot \mathbf{k}
= \mathbf{q}^\top R(\theta_k - \theta_q) \cdot \mathbf{k}
$$

即：

> **RoPE 实现了 “绝对位置编码方式得到的相对位置感知”**：注意力变成了与 $(n - m)$（即位置差）相关的点积结果。

---

### 示例（假设 $\omega = 1$, $m = 1$, $n = 2$）

- 则 $\theta_q = 1$, $\theta_k = 2$
- $R(\theta_k - \theta_q) = R(1)$

那么有：

```python
import numpy as np

q = np.array([1, 2])
k = np.array([3, 4])

# 相对旋转角
theta = 1.0  # θ_k - θ_q
R = np.array([
    [np.cos(theta), -np.sin(theta)],
    [np.sin(theta),  np.cos(theta)],
])

k_rot = R @ k
att_score = q @ k_rot
print("注意力得分（RoPE）:", att_score)# 7.62626733416533，是一个数，代表了q向量中的第m个元素和k向量中的第n个元素的注意力得分。
```

事实上，RoPE将每对特征维度（比如 [x₀, x₁]）看作是二维平面上的一个点(x₀, x₁)，然后将其绕原点(0, 0)顺时针或逆时针旋转一个角度θ（由位置pos决定），这个操作的数学本质就是二维向量绕原点旋转。

这个旋转中心正是(0, 0)。所以可以想象：特征 [x0, x1] 像一个在平面上的箭头，RoPE 让它随着token的位置pos增大不断绕原点旋转，旋转角度=pos × freq_i。
![](D:\MyFile\github\minimind-master\notes\rope_rotation.gif)

# 三、推广到高维向量(词向量,d_model维,d_model >> 2)
上面介绍了当向量为二维时，RoPE的工作原理。

当向量维度非常高时，比如词向量的维度，可以**把高维向量中不同位置(i)的元素两两分成一组，分别执行旋转操作**，如下：

$$
\boldsymbol{R}_{\Theta,m}^{d_{model}} \boldsymbol{x} =
\begin{pmatrix}
\cos m\theta_0 & -\sin m\theta_0 & 0 & 0 & \cdots & 0 & 0 \\
\sin m\theta_0 & \cos m\theta_0 & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos m\theta_2 & -\sin m\theta_2 & \cdots & 0 & 0 \\
0 & 0 & \sin m\theta_2 & \cos m\theta_2 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos m\theta_{d_{model}-2} & -\sin m\theta_{d_{model}-2} \\
0 & 0 & 0 & 0 & \cdots & \sin m\theta_{d_{model}-2} & \cos m\theta_{d_{model}-2}
\end{pmatrix}
\begin{pmatrix} 
x_0 \\
x_1 \\
x_2 \\
\vdots \\
x_{d_{model}-2} \\
x_{d_{model}-1}
\end{pmatrix}
$$


其中，$\Theta=\left\{\theta_i=\omega^{-\frac{i}{d_{model}}}, i \in [0, 2, \ldots, d_{model}-2] \right\}$。

注意，公式中的m指的是pos，即序列中第m个词向量的pos=m，之所以不用pos，是为了使得公式看起来简洁。

这个高维旋转矩阵是高度稀疏的，在代码实现时，通常改写成如下方式进行替代，以减少冗余计算：

$$
\boldsymbol{R}_{\Theta,m}^{d_{model}} \boldsymbol{x} = 
\begin{pmatrix}
x_{0} \\
x_{1} \\
x_{2} \\
x_{3} \\
\vdots \\
x_{d_{model}-2} \\
x_{d_{model}-1}
\end{pmatrix} 
\otimes 
\begin{pmatrix} 
\cos m\theta_0 \\
\cos m\theta_0 \\
\cos m\theta_2 \\
\cos m\theta_2 \\
\vdots \\
\cos m\theta_{d_{model}-2} \\
\cos m\theta_{d_{model}-2}
\end{pmatrix}
+ 
\begin{pmatrix}
-x_{1} \\
x_{0} \\
-x_{3} \\
x_{2} \\
\vdots \\
-x_{d_{model}-1} \\
x_{d_{model}-2}
\end{pmatrix} 
\otimes 
\begin{pmatrix} 
\sin m\theta_0 \\
\sin m\theta_0 \\
\sin m\theta_2 \\
\sin m\theta_2 \\
\vdots \\
\sin m\theta_{d_{model}-2} \\
\sin m\theta_{d_{model}-2}
\end{pmatrix}
$$

可以看到，经过RoPE，词向量的维度不变（仍为$d_{model}$）。

# 四、高维RoPE 示例：`d_model = 4`, `omiga = 10000`, `pos = 2`

假设原始向量（如query向量）为：

$$
\mathbf{q} = [1,\ 0,\ 0,\ 1]
$$

---

#### 第一步：计算频率

对于 $d_{model} = 4$，每2个维度一对：

$$
\text{freqs} = \left[ \frac{1}{10000^{0/4}},\ \frac{1}{10000^{2/4}} \right] = [1.0,\ 0.01]
$$

位置 $pos = 2$ 时，对应旋转角度为：

$$
\theta_0 = 2 \cdot 1.0 = 2.0 \\
\theta_1 = 2 \cdot 0.01 = 0.02
$$

---

#### 第二步：按维度对进行旋转

第1对维度 $(1,\ 0)$，角度 $2.0$：

$$
\begin{bmatrix}
\cos(2) & -\sin(2) \\
\sin(2) & \cos(2)
\end{bmatrix}
\cdot
\begin{bmatrix}
1 \\
0
\end{bmatrix}
=
\begin{bmatrix}
\cos(2) \\
\sin(2)
\end{bmatrix}
\approx
\begin{bmatrix}
-0.4161 \\
0.9093
\end{bmatrix}
$$

第2对维度 $(0,\ 1)$，角度 $0.02$：

$$
\begin{bmatrix}
\cos(0.02) & -\sin(0.02) \\
\sin(0.02) & \cos(0.02)
\end{bmatrix}
\cdot
\begin{bmatrix}
0 \\
1
\end{bmatrix}
=
\begin{bmatrix}
-\sin(0.02) \\
\cos(0.02)
\end{bmatrix}
\approx
\begin{bmatrix}
-0.02 \\
0.9998
\end{bmatrix}
$$

---

#### 第三步：RoPE 编码后向量

拼接两对旋转结果：

$$
\text{RoPE}(\mathbf{q}) = [-0.4161,\ 0.9093,\ -0.02,\ 0.9998]
$$

---

PyTorch 验证代码：
```python
import torch

d_model = 4
dd=d_model//2
omiga = 10000.0
m = 2

freqs = 1.0 / (omiga ** (torch.arange(0, d_model, 2).float() / d_model))
angles = m * freqs  # [2.0, 0.02]

q = torch.tensor([1.0, 0.0, 0.0, 1.0])

cos = torch.cat([torch.cos(angles), torch.cos(angles)])
sin = torch.cat([torch.sin(angles), torch.sin(angles)])

def rotate_half(x):
    return torch.cat([-x[dd:], x[:dd]])

q_embed = q * cos + rotate_half(q) * sin
print(q_embed)# tensor([-0.4161, -0.0200,  0.9093,  0.9998])，和手动计算的一致！
```

# 五、代码实现RoPE
https://zhuanlan.zhihu.com/p/645263524

在MiniMind中实现的RoPE参考了Transformers库的LLaMA模型中RoPE的实现方式，和上述公式有些区别，具体体现在：

假设输入高维向量为q=[1,2,3,4,5,6]
1. 应用上述公式做rotate_half(q)            --> [-2,  1, -4,  3, -6,  5]
2. 应用MiniMind/LLaMa的方法做rotate_half(q)      -->[-4, -5, -6,  1,  2,  3]

在这个链接中，证明了两者的等价性： https://discuss.huggingface.co/t/is-llama-rotary-embedding-implementation-correct/44509/2


这里我们展示MiniMind中的RoPE实现代码：
```python
def precompute_freqs_cis(d_model: int, end: int = int(32 * 1024), omiga: float = 1e6):
    freqs = 1.0 / (omiga ** (torch.arange(0, d_model, 2)[: (d_model // 2)].float() / d_model))
    t = torch.arange(end, device=freqs.device)# end是最长预计算freqs的长度，可任意扩增
    freqs = torch.outer(t, freqs).float()# 外积×：[end x 1, 1 x d_model//2)-->end x d_model//2 , 得到 "每个位置 × 每个频率" 的角度 θ
    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)# end x d_model//2--> end x d_model
    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)# end x d_model//2--> end x d_model
    return freqs_cos, freqs_sin

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    def rotate_half(x):
        return torch.cat((-x[..., x.shape[-1] // 2:], x[..., : x.shape[-1] // 2]), dim=-1)

    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))
    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))
    return q_embed, k_embed

d_model=4
q=torch.tensor([1,2,3,4])
k=torch.tensor([5,6,7,8])

freqs_cos, freqs_sin = precompute_freqs_cis(d_model)
q_embed, k_embed=apply_rotary_pos_emb(q,k,freqs_cos, freqs_sin)
print(q_embed.shape, k_embed.shape)# torch.Size([32768, 1, 4]) torch.Size([32768, 1, 4])   1是维度扩展得到的，4是d_model，32768是当前设置的最长序列长度
```

